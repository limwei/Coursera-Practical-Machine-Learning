
Setting some initial settings - packages and other documents.
```{r}
library(ggplot2)
library(caret)
library(rpart)
library(RColorBrewer)
library(rpart.plot)
library(randomForest)
library(rattle)
```

```{r}
train <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), na.strings = c("", "NA", "#DIV/0"))
test <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), na.strings = c("", "NA", "#DIV/0"))

```

I'm now going to partition the training set into two to run my analysis.

```{r}
trainPart.ind <- createDataPartition(train$classe, p=0.6, list=FALSE)
trainPart <- train[trainPart.ind,]
testPart <- train[-trainPart.ind,]

dim(trainPart)
dim(testPart)
```

Let's start data cleaning.

```{r}
## Remove NearZeroVariance variables
trainPart.nzv <- nearZeroVar(trainPart, saveMetrics=TRUE)
trainPart <- trainPart[,trainPart.nzv$nzv==FALSE]

testPart.nzv <- nearZeroVar(testPart, saveMetrics=TRUE)
testPart <- testPart[,testPart.nzv$nzv==FALSE]

## Remove id variable
trainPart <- trainPart[,-1]

## Clean variables that have more than 60% NAs

ind.NA <- which(colSums(is.na(trainPart))/nrow(trainPart)>0.6)
trainPart <- trainPart[,-ind.NA]


```

Now transform testPart (partitioned training set) and test (testing set) datasets

```{r}
testPart <- testPart[,colnames(trainPart)] # remove the classe column
test <- test[,colnames(trainPart)[-58]]

dim(testPart)
dim(test)

```

Coerce data into the same type

```{r}
for (i in 1:length(test)){
  for(j in 1:length(trainPart)){
    if (length(grep(names(trainPart[i]), names(test)[j])) == 1) { class(test[j]) <- class(trainPart[i])}
  }
}

# to make sure i've coerced correctly, i'm gonna add rows from my trainPart to test. 

test <- rbind(trainPart[19, -58], test)
test <- test[-1,] ## remove the extra row added

```

**Prediction with Decision Trees**

```{r}

set.seed(11911)
mod_rpart <- rpart(classe ~ ., data=trainPart, method="class")

fancyRpartPlot(mod_rpart)

# let's see how the predictions do.

pred_rpart <- predict(mod_rpart, testPart, type="class")
cfmat_rpart <- confusionMatrix(pred_rpart, testPart$classe)
cfmat_rpart

```

Overall accuracy is 86.3%. Will try something else.

**Prediction with Random Forests**

```{r}
set.seed(11911)
mod_rf <- randomForest(classe ~ ., data=trainPart)
pred_rf <- predict(mod_rf, testPart, type="class")
cfmat_rf <- confusionMatrix(pred_rf, testPart$classe)
cfmat_rf

plot(mod_rf) ## diagnostics suggest convergence.

````
Overall accuracy is at 99.8%, very high.

**  Prediction with GBMs **

```{r}
set.seed(11911)
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 1)

mod_gbm <- train(classe ~., data=trainPart, method="gbm", trControl = fitControl, verbose=FALSE)

mod_gbm_final <- mod_gbm$finalModel

pred_gbm <- predict(mod_gbm, newdata=testPart)
cfmat_gbm <- confusionMatrix(pred_gbm, testPart$classe)

cfmat_gbm

```

Random Forests give the best predictions. So we'll now predict the test set

```{r}
finalpred <- predict(mod_rf, test, type="class")
finalpred
```